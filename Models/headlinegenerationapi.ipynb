{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8036227,"sourceType":"datasetVersion","datasetId":4737418},{"sourceId":8252776,"sourceType":"datasetVersion","datasetId":4897055},{"sourceId":8265446,"sourceType":"datasetVersion","datasetId":4906560},{"sourceId":8175589,"sourceType":"datasetVersion","datasetId":4839372},{"sourceId":8266541,"sourceType":"datasetVersion","datasetId":4907428}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T19:04:59.442325Z","iopub.execute_input":"2024-04-29T19:04:59.442784Z","iopub.status.idle":"2024-04-29T19:04:59.456835Z","shell.execute_reply.started":"2024-04-29T19:04:59.442743Z","shell.execute_reply":"2024-04-29T19:04:59.455654Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"/kaggle/input/model-till-32000/config.json\n/kaggle/input/model-till-32000/merges.txt\n/kaggle/input/model-till-32000/vocab.json\n/kaggle/input/model-till-32000/tokenizer_config.json\n/kaggle/input/model-till-32000/model.safetensors\n/kaggle/input/model-till-32000/special_tokens_map.json\n/kaggle/input/model-till-32000/added_tokens.json\n/kaggle/input/model-till-32000/generation_config.json\n/kaggle/input/news-2-0/news_2.0.csv\n/kaggle/input/model-till-25000/config.json\n/kaggle/input/model-till-25000/merges.txt\n/kaggle/input/model-till-25000/vocab.json\n/kaggle/input/model-till-25000/tokenizer_config.json\n/kaggle/input/model-till-25000/model.safetensors\n/kaggle/input/model-till-25000/special_tokens_map.json\n/kaggle/input/model-till-25000/added_tokens.json\n/kaggle/input/model-till-25000/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline\n\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T19:05:04.342666Z","iopub.execute_input":"2024-04-29T19:05:04.343088Z","iopub.status.idle":"2024-04-29T19:05:04.349667Z","shell.execute_reply.started":"2024-04-29T19:05:04.343054Z","shell.execute_reply":"2024-04-29T19:05:04.348675Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/news-2-0/news_2.0.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T19:05:05.350183Z","iopub.execute_input":"2024-04-29T19:05:05.350603Z","iopub.status.idle":"2024-04-29T19:05:05.355181Z","shell.execute_reply.started":"2024-04-29T19:05:05.350570Z","shell.execute_reply":"2024-04-29T19:05:05.354047Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom transformers import pipeline\n\nmodel_ckpt = \"facebook/bart-large-cnn\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipe = pipeline('summarization', model=model_ckpt, device = device)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T19:05:06.219862Z","iopub.execute_input":"2024-04-29T19:05:06.220219Z","iopub.status.idle":"2024-04-29T19:05:16.491486Z","shell.execute_reply.started":"2024-04-29T19:05:06.220190Z","shell.execute_reply":"2024-04-29T19:05:16.490455Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dcfb0d386ed4577b0a78673b5df3311"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e587b074f7e4d0abd6e35b7dfac7e96"}},"metadata":{}}]},{"cell_type":"code","source":"# Example function to generate segments of text within the maximum sequence length\ndef generate_segments(input_text):\n    words = input_text.split()\n    for i in range(0, len(words), 300):  # Adjust the segment length here\n        yield ' '.join(words[i:i+300])   # Adjust the segment length here\n\ndef summarization(input_text):\n    global count\n    if(len(tokenizer.tokenize(input_text))<=1020):\n        count += 1\n        print(count, end=\" \")\n        return input_text\n\n    output_summary = \"\"\n    for segment in generate_segments(input_text):\n        try:\n            max_length = len(segment.split()) # Adjust max_length dynamically\n            summarized_segment = pipe(segment, max_length=max_length, min_length=1, do_sample=False)[0]['summary_text']\n            output_summary += summarized_segment\n            \n        except IndexError:\n            pass\n        except Exception as e:\n            print(f\"Error occurred: {e}\")\n    if len(output_summary.split()) >= 1020:\n            output_summary = summarization(output_summary)\n    count += 1\n    print(count, end=\" \")\n    return output_summary\ncount = 0","metadata":{"execution":{"iopub.status.busy":"2024-04-29T19:05:21.960419Z","iopub.execute_input":"2024-04-29T19:05:21.961361Z","iopub.status.idle":"2024-04-29T19:05:21.971164Z","shell.execute_reply.started":"2024-04-29T19:05:21.961324Z","shell.execute_reply":"2024-04-29T19:05:21.969860Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel_path = \"/kaggle/input/model-till-12000\"\n\n# Load the model\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\ntokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# If GPU is available, move the model to GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:08:34.461337Z","iopub.execute_input":"2024-04-29T20:08:34.461757Z","iopub.status.idle":"2024-04-29T20:08:49.725463Z","shell.execute_reply.started":"2024-04-29T20:08:34.461725Z","shell.execute_reply":"2024-04-29T20:08:49.724466Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebed8ec8a4e643b1bbe1477d0332c325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06148e28bae0472892dbbfcd6ef047d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b809785e5b674dbf87675ad11a304189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807d8066119046c8ade2de0fe616854c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a0487eed3274d50b67d9bc01731e3c8"}},"metadata":{}},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50258, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"!pip install flask-ngrok >> /dev/null\n!pip install flask_cors\n!pip install pyngrok\nfrom flask import Flask,request,jsonify\nfrom pyngrok import ngrok\nfrom flask_cors import CORS\nfrom flask import Response\n!ngrok config add-authtoken 2cDjv295789xMxfOXzNy20lFY2C_2zrPAEF32U1teXtTXx2rr\nngrok.set_auth_token(\"2cDjv295789xMxfOXzNy20lFY2C_2zrPAEF32U1teXtTXx2rr\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T19:05:32.068694Z","iopub.execute_input":"2024-04-29T19:05:32.069407Z","iopub.status.idle":"2024-04-29T19:06:15.975911Z","shell.execute_reply.started":"2024-04-29T19:05:32.069360Z","shell.execute_reply":"2024-04-29T19:06:15.974735Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Requirement already satisfied: flask_cors in /opt/conda/lib/python3.10/site-packages (4.0.0)\nRequirement already satisfied: Flask>=0.9 in /opt/conda/lib/python3.10/site-packages (from flask_cors) (3.0.3)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (3.0.2)\nRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (3.1.2)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (8.1.7)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask>=0.9->flask_cors) (1.7.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->Flask>=0.9->flask_cors) (2.1.3)\nRequirement already satisfied: pyngrok in /opt/conda/lib/python3.10/site-packages (7.1.6)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.1)\nAuthtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"}]},{"cell_type":"code","source":"def generation(query):\n    headline = []\n    input_ids = tokenizer(query, return_tensors='pt')\n    # Move the input to GPU if available\n    input_ids = input_ids.to(\"cuda\")\n\n    # Generate 5 different outputs\n    num_outputs = 7\n    generated_outputs = []\n    max_new_tokens = 24\n    for _ in range(num_outputs):\n        # Generate output using the model\n        output = model.generate(\n            input_ids=input_ids[\"input_ids\"],\n            attention_mask=input_ids['attention_mask'],\n            length_penalty=0.8,\n            min_new_tokens=5,\n            max_new_tokens=max_new_tokens+_,\n            num_beams=8,\n            no_repeat_ngram_size=2,\n            early_stopping=True,\n            do_sample=True\n        )\n        generated_outputs.append(tokenizer.decode(output[0]))\n    for i, output_text in enumerate(generated_outputs):\n        # Remove <pad> tokens\n        output_text = output_text.replace(\"<pad>\", \"\")\n        output_text = output_text.replace(\"<|endoftext|>\", \"\")\n\n        # Find the index of \"TL;DR:\"\n        tldr_index = output_text.find(\"TL;DR:\")\n        if tldr_index != -1:\n            # Extract the text after \"TL;DR:\"\n            output_text = output_text[tldr_index + len(\"TL;DR:\"):]\n\n        # Remove commas from the output text\n        output_text = output_text.replace(\",\", \"\")\n        headline.append(output_text.strip())\n    return headline\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:09:14.379504Z","iopub.execute_input":"2024-04-29T20:09:14.379902Z","iopub.status.idle":"2024-04-29T20:09:14.389752Z","shell.execute_reply.started":"2024-04-29T20:09:14.379870Z","shell.execute_reply":"2024-04-29T20:09:14.388789Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"app = Flask(__name__)\nCORS(app)\n\n@app.before_request\ndef basic_authentication():\n    if request.method.lower() == 'options':\n        return Response()\n\n\n@app.route(\"/\")\ndef home():\n    return jsonify({'result': \"result\"})\n\n@app.route('/api/data/',methods=['POST'])\ndef get_data():\n    article = request.get_json()\n    sentence = summarization(article)\n    query = sentence + '\\nTL;DR:'\n    result = generation(query)\n    return result\n\nngrok_tunnel = ngrok.connect(5000)\nprint(' * Tunnel URL:', ngrok_tunnel.public_url)\n\nif __name__ == \"__main__\":\n    app.run()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:09:17.340280Z","iopub.execute_input":"2024-04-29T20:09:17.340803Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":" * Tunnel URL: https://c2ba-34-73-219-99.ngrok-free.app\n * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"26 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"27 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"28 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"29 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"30 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"31 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"32 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"33 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"34 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"35 ","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}]}]}